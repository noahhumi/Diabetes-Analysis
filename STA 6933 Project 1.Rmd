---
title: "Project 1"
author: "Noah Miller"
date: "2025-03-22"
output: html_document
---

```{r}
library(dplyr)
library(gam)
library(mgcv)
library(splines)
library(boot)
library(tree)
library(leaps)
library(randomForest)
library(gbm)

diab_data <- read.csv("C:/Users/noahm/OneDrive/Documents/R/STA 6933/diabetes.csv")
# The following lines filter out rows where certain values equal zero when it isn't possible.
diab_data <- diab_data %>% filter(BMI > 0)
diab_data <- diab_data %>% filter(SkinThickness > 0)
diab_data <- diab_data %>% filter(BloodPressure > 0)
diab_data <- diab_data %>% filter(Glucose > 0)
diab_data <- diab_data %>% filter(Insulin > 0)
```
Patients were removed if BMI, skin thickness, blood pressure, glucose or insulin were zero as they are not physically possible values for a person to have.

# Training/Test Data
```{r}
# This code chunk splits the data into training data and testing data.
set.seed(123)
train = sample(nrow(diab_data), 0.7*nrow(diab_data))
diab.train = diab_data[train,]
diab.test = diab_data[-train,]
# The following line uses forward stepwise selection on the training set.
diab.forward <- regsubsets(Outcome ~ ., data = diab.train, nvmax = 17, method = "forward")
diab_sum <- summary(diab.forward)
```

# Generalized Linear Model
```{r}
set.seed(1)
glm.def <- glm(Outcome ~ ., data = diab.train, family = binomial)
summary(glm.def)
```
Glucose is by far the most statistically significant predictor, whereas BMI, Age and DiabetesPedigreeFunction are also statistically significant.
```{r}
# This determines the mean squared error of the generalized linear model.
pred.glm <- predict(glm.def, diab.test, type = "response")
pred.glm <- ifelse(pred.glm > 0.5, 1, 0)
mean(pred.glm != diab.test$Outcome)
```
The generalized linear model has a 18.64% training error.

# GAM Model
```{r}
# This determines the best number of predictors to use.
par(mfrow=c(2,2))
plot(diab_sum$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
index = which.min(diab_sum$cp) # 4 predictors
points(index,diab_sum$cp[index],col="red",cex=2,pch=20)

plot(diab_sum$bic ,xlab="Number of Variables ",ylab="BIC",type='l')
index = which.min(diab_sum$bic) # 3 predictors
points(index,diab_sum$bic[index],col="red",cex=2,pch=20)

plot(diab_sum$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
index = which.max(diab_sum$adjr2) # 4 predictors
points(index,diab_sum$adjr2[index], col="red",cex=2,pch=20)
```

While Cp and the adjusted R^2 have 4 predictors, BIC says that 3 is the best number to use.

```{r}
# This gives the 3 predictors that are best to use with the model.
coef(diab.forward, 3)
```
```{r}
gam_model <- gam(Outcome ~ s(Glucose) + s(BMI) + s(Age), data=diab.train)
par(mfrow = c(2, 2))
plot(gam_model, se = TRUE, col = "blue")
```

Glucose and BMI have a positive linear relationship with a diabetes diagnosis. Age does not appear to have a strong linear relationship with the outcome.

```{r}
# These functions print out the MSE and R^2 values
preds <- predict(gam_model, diab.test)
mse <- mean((diab.test$Outcome - preds)^2)
c("Training error using decision tree",mse)
```
The mean squared error is about 0.15, which means the predicted values are roughly close to the actual values in the GAM model.

```{r}
summary(gam_model)
```
The summary of the GAM model suggests that each of the 3 predictors used are statistically significant.

# GAM Model - 4 Predictors
```{r}
# This gives the 4 predictors that are best to use with the model.
coef(diab.forward, 4)
```
```{r}
gam_model <- gam(Outcome ~ s(Glucose) + s(BMI) + s(Age) + s(DiabetesPedigreeFunction), data=diab.train)
par(mfrow = c(2, 2))
plot(gam_model, se = TRUE, col = "blue")
```

Glucose, BMI and Age still have the same relationship with the outcome as when the model only used 3 predictors. DiabetesPedigreeFunction does not appear to have a strong linear relationship with the outcome similar to Age.
```{r}
# These functions print out the MSE and R^2 values
preds <- predict(gam_model, diab.test)
mse <- mean((diab.test$Outcome - preds)^2)
c("Training error using decision tree",mse)
```
The mean squared error is about 0.15 but slightly higher than the 3-predictor model.

```{r}
summary(gam_model)
```
DiabetesPedigreeFunction is not as statistically significant as the other 3 predictors.

# Decision Trees
```{r}
# This tree is made based off the training data set.
set.seed(1)
diab.tree = tree(as.factor(Outcome) ~ ., data = diab.train)
dectree.pred = predict(diab.tree, newdata = diab.train, type = "class")
c("Training error using decision tree",mean(dectree.pred != diab.train$Outcome))
plot(diab.tree)
text(diab.tree)
```

The decision tree suggests that when Glucose is high, a patient is more likely to be diabetic, especially when other values tend to be high alongside the higher glucose levels. The decision tree has a 10.22% training error.

# Bagging and Random Forest
```{r}
set.seed(21)
bag.diab <- randomForest(as.factor(Outcome) ~ ., data = diab.train, ntree = 25, mtry = 8)
bag.pred <- predict(bag.diab, newdata = diab.train, type = "class")
c("Training error using bagging",mean(bag.pred != diab.train$Outcome))
importance(bag.diab)
varImpPlot(bag.diab)
```

Glucose is the most important predictor in the data set based off the results of bagging, which has a 0.36% training error.

# Boosting
```{r}
set.seed(123)
boost.diab = gbm(Outcome ~ ., data = diab.train, distribution = 'bernoulli', n.trees = 500)
boost.pred = predict(boost.diab, n.trees = 25, type = "response")
boost.pred = ifelse(boost.pred>0.5,1,0)
c("Training error using boosting",mean(boost.pred != diab.train$Outcome))
summary(boost.diab)
```
The results of boosting suggest Glucose is an important predictor in the data set. The training error is 20.8%

# Splines
```{r}
# Glucose
set.seed(314)
partial = sample(seq_len(nrow(diab_data)),floor(0.1*nrow(diab_data)))
diab_data1 = diab_data[partial,c("Glucose","Outcome")]
xr = range(diab_data1$Glucose)
xs = seq(xr[1],xr[2],length.out = 200)

lm.sp3 = lm(Outcome ~ bs(Glucose, knots = 135, degree = 3), data = diab_data1)
lm.sp1 = lm(Outcome ~ bs(Glucose, knots = 135, degree = 1), data = diab_data1)
plot(diab_data1$Glucose,diab_data1$Outcome, xlab = "Glucose Level", ylab = "Outcome", cex=.5, col="darkgrey", main = "Linear and Cubic Splines")
lines(xs, predict(lm.sp3, data.frame(Glucose = xs)), col = "red")
lines(xs, predict(lm.sp1, data.frame(Glucose = xs)), col = "blue")
abline(v = 135, col = "green",lty = 3)
legend("topright",legend = c("linear","polynomial with degree 3"),col = c("blue","red"),lty = 1)
```

The cubic and linear splines both gradually increase, suggesting glucose levels increasing lead to a higher chance of having diabetes.

```{r}
# BMI
set.seed(314)
partial = sample(seq_len(nrow(diab_data)),floor(0.1*nrow(diab_data)))
diab_data1 = diab_data[partial,c("BMI","Outcome")]
xr = range(diab_data1$BMI)
xs = seq(xr[1],xr[2],length.out = 200)

lm.sp3 = lm(Outcome ~ bs(BMI, knots = 36, degree = 3), data = diab_data1)
lm.sp1 = lm(Outcome ~ bs(BMI, knots = 36, degree = 1), data = diab_data1)
plot(diab_data1$BMI,diab_data1$Outcome, xlab = "BMI Level", ylab = "Outcome", cex=.5, col="darkgrey", main = "Linear and Cubic Splines")
lines(xs, predict(lm.sp3, data.frame(BMI = xs)), col = "red")
lines(xs, predict(lm.sp1, data.frame(BMI = xs)), col = "blue")
abline(v = 36, col = "green",lty = 3)
legend("topright",legend = c("linear","polynomial with degree 3"),col = c("blue","red"),lty = 1)
```

The splines both increase until a certain point before decreasing.

```{r}
# Age
set.seed(314)
partial = sample(seq_len(nrow(diab_data)),floor(0.1*nrow(diab_data)))
diab_data1 = diab_data[partial,c("Age","Outcome")]
xr = range(diab_data1$Age)
xs = seq(xr[1],xr[2],length.out = 200)

lm.sp3 = lm(Outcome ~ bs(Age, knots = 35, degree = 3), data = diab_data1)
lm.sp1 = lm(Outcome ~ bs(Age, knots = 50, degree = 1), data = diab_data1)
plot(diab_data1$Age,diab_data1$Outcome, xlab = "Age of Patients", ylab = "Outcome", cex=.5, col="darkgrey", main = "Linear and Cubic Splines")
lines(xs, predict(lm.sp3, data.frame(Age = xs)), col = "red")
lines(xs, predict(lm.sp1, data.frame(Age = xs)), col = "blue")
abline(v = 50, col = "green",lty = 3)
legend("topright",legend = c("linear","polynomial with degree 3"),col = c("blue","red"),lty = 1)
```

Similar to BMI, the splines start off increasing until they both start to decrease, which means Age and BMI are not as influential as Glucose is.